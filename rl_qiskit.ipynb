{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2fa771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from qiskit import QuantumCircuit, transpile\n",
    "from qiskit_aer import AerSimulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6119536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, map_name=\"4x4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning parameters\n",
    "n_actions = env.action_space.n\n",
    "n_states = env.observation_space.n\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.95\n",
    "epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.995\n",
    "episodes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711cc4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum backend\n",
    "backend = AerSimulator()\n",
    "\n",
    "def quantum_reward_modulation():\n",
    "    \"\"\"Quantum circuit to modulate rewards.\"\"\"\n",
    "    qc = QuantumCircuit(1, 1)\n",
    "    # Random angles for a single-qubit unitary\n",
    "    theta = np.random.uniform(0, np.pi)\n",
    "    phi = np.random.uniform(0, np.pi)\n",
    "    lam = np.random.uniform(0, np.pi)\n",
    "    \n",
    "    qc.u(theta, phi, lam, 0)\n",
    "    qc.measure(0, 0)\n",
    "\n",
    "    # Transpile and run the circuit\n",
    "    transpiled_qc = transpile(qc, backend)\n",
    "    job = backend.run(transpiled_qc, shots=100)\n",
    "    result = job.result()\n",
    "    counts = result.get_counts()\n",
    "\n",
    "    # Calculate probability of measuring '0'\n",
    "    p0 = counts.get('0', 0) / 100\n",
    "    return p0  # Returns a value in [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for episode in range(episodes):\n",
    "    state = env.reset()[0]  # Extract the state from the reset tuple\n",
    "    done = False\n",
    "    truncated = False\n",
    "\n",
    "    while not done and not truncated:\n",
    "        # Exploration vs Exploitation\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "\n",
    "        # Step in the environment\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Quantum reward modifier\n",
    "        modifier = quantum_reward_modulation()\n",
    "        modified_reward = reward * (1 + 0.1 * modifier)  # Slight reward scaling\n",
    "\n",
    "        # Q-Learning Update\n",
    "        q_table[state, action] += learning_rate * (\n",
    "            modified_reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
    "        )\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # Decay exploration\n",
    "    epsilon = max(min_epsilon, epsilon * decay_rate)\n",
    "\n",
    "    # Optional: Print progress every 100 episodes\n",
    "    if (episode + 1) % 100 == 0:\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Epsilon: {epsilon:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16733b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training finished.\\n\")\n",
    "print(\"Q-table:\")\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7b48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Evaluate the learned policy\n",
    "def evaluate_policy(n_eval_episodes=100):\n",
    "    successes = 0\n",
    "    for _ in range(n_eval_episodes):\n",
    "        state = env.reset()[0]  # Extract the state\n",
    "        done = False\n",
    "        truncated = False\n",
    "        while not done and not truncated:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            if reward == 1.0:  # Reached the goal\n",
    "                successes += 1\n",
    "                break\n",
    "    print(f\"\\nEvaluation: Success rate = {successes / n_eval_episodes * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b8c6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_policy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qua",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
